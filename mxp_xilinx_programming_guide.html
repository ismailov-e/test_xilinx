<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
<link type="text/css" rel="stylesheet" href="css/bootstrap.css"/>
<link type="text/css" rel="stylesheet" href="css/jquery.ui.all.css"/>
<link type="text/css" rel="stylesheet" href="css/jquery.tocify.css"/>
<style>
body {
    padding-top: 20px;
}
p {
    font-size: 16px;
}
.headerDoc {
    color: #005580;
}

@media (max-width: 767px) {
    #toc {
        position: relative;
        width: 100%;
        margin: 0px 0px 20px 0px;
    }
}
</style>
</head>
<body>
<div class="container-fluid">
<div class="row-fluid">
<div class="span3">
<div id="toc"></div>
</div>
<div class="span9">
<h1 id="vectorblox_mxp">VectorBlox MXP</h1>
<div class="figure">
<img src="images/microblaze-mxp.png" alt="VectorBlox MXP System with Microblaze" /><p class="caption">VectorBlox MXP System with Microblaze</p>
</div>
<div class="figure">
<img src="images/nios-mxp.png" alt="VectorBlox MXP System with Nios II" /><p class="caption">VectorBlox MXP System with Nios II</p>
</div>
<h2 id="architecture_overview">Architecture Overview</h2>
<p>The VectorBlox MXP matrix processor is an extremely high-performance processor capable of speedups in excess of 1000<span class="math"> × </span> faster than MicroBlaze or Nios II/f. The design of the processor was inspired by the vector processors used in scientific supercomputers made by Cray, Fujitsu and NEC. However, the MXP is not a simple clone of one of these processors. It has been redesigned from the ground up to perform well on embedded applications which operate primarily on various integer widths and fixed-point data types. It has also been designed from the start to map exceptionally well into modern FPGAs in a way that exploits their hard RAM blocks and hard multiplier or DSP blocks.</p>
<p>The VectorBlox MXP matrix processor is an update on the classical vector processor. Instead of operating merely on vectors, it can also operate on 2D and 3D matrices. It performs parallel calculations directly on sets of data stored directly in a private scratchpad memory, rather than a register file.</p>
<p>To achieve speedups in excess of 1000<span class="math"> × </span>, the VectorBlox MXP employs several strategies to maximize parallelism and to reduce overhead such as address calculations. This ensures the parallel ALUs employed by the MXP are working to their full potential on actual data calculations.</p>
<p>The parallelism available in MXP exceeds that of traditional scalar CPUs, fixed-width SIMD operations, and even variable-length vector CPUs. This goes beyond the number of parallel ALUs employed, as we have witnessed speedups of 20<span class="math"> × </span> or higher using just a single vector lane! Here are some of the reasons why you may expect speedups that exceed the number of ALUs on your own code:</p>
<ul>
<li><p>Unlike scalar CPUs or fixed-width SIMD operations, loop counters and branches are not necessary and can be eliminated from the inner loop. This eliminates the overhead of counting/incrementing, comparisons, conditional branches, and branch mispredictions.</p></li>
<li><p>Unlike traditional scalar vector CPUs, each ALU in MXP can perform 2 parallel calculations on halfwords (16-bit integers) or 4 parallel calculations on bytes (8-bit integers).</p></li>
<li><p>Unlike scalar CPUs or fixed-width SIMD operations, load and store instructions are not necessary and can be eliminated from the inner loop. These normally transfer data from a cache to the register file, and may even result in a cache miss. Instead, the MXP operates memory-to-memory on data already in the scratchpad and never suffers from a cache miss.</p></li>
<li><p>Traditional vector CPUs support a fixed number of named vectors, each with a maximum size. Instead, the MXP can partition its large and flexible scratchpad arbitrarily into any number of vectors, of any length, starting at any address, that is subject only to overall scratchpad capacity. This improves overall data availability and significantly reduces data copying.</p></li>
<li><p>The MXP scratchpad is easily double-buffered, allowing all memory latency to be hidden.</p></li>
</ul>
<h2 id="scratchpad">Scratchpad</h2>
<p>The MXP does not operate on data directly stored in memory. Instead, data must first be DMA-transferred into a private local memory called a scratchpad. Like a cache, the scratchpad is designed to provide fast, parallel access to data. However, unlike a cache, the scratchpad is not managed automatically for the programmer. Instead, the programmer must explicitly transfer data from host to scratchpad, or from scratchpad to host.</p>
<p>The scratchpad is byte addressable. A vector, matrix, or submatrix is identified explicitly by a pointer to its starting address in the scratchpad. The contents of a vector are striped across several parallel block memories, allowing full parallel readout when a vector is accessed sequentially. All vector operations start at the lowest address and proceed to the highest address, subject to the length of the vector. Operations on long vectors are carried out over multiple clock cycles, one <strong><em>wavefront</em></strong> of data at a time. No matter what the starting address (aligned or not), a full wavefront which spans the full width of the scratchpad can always be read in one clock cycle.</p>
<h2 id="data_organization">Data Organization: Vectors, 2D Matrices and 3D Matrices</h2>
<p>The most efficient way to use MXP is to organize data in memory contiguously into vectors, or as packed 2D or packed 3D matrices.</p>
<p>By a 2D packed matrix, we mean a series of rows of data that are placed end-to-end, possibly with some fixed amount of padding between each pair of adjacent rows. There must be a constant difference between the starting addresses of any two adjacent rows.</p>
<p>By a 3D packed matrix, we mean a series of 2D matrices that are placed end-to-end, possibly with some fixed amount of padding between each pair of 2D adjacent matrices. There must be a constant difference between the starting addresses of any two adjacent 2D matrices.</p>
<p>Before operating on any data, the MXP processor must also be told the size or dimensions of the vector or matrix. This information is provided to the processor in advance of the instructions and remembered in its configuration state.</p>
<p>The minimal information required for vector instructions is the <strong><em>vector length</em></strong>. As will be described later, more information is also required for 2D and 3D matrix operations, such as the number of rows.</p>
<h2 id="vector_instructions">Vector Instructions</h2>
<p>A typical instruction is provided with three explicit operands: pointers to the destination, to the source operand A, and to the source operand B. In addition, type information is explicitly provided to specify the data element size (byte, halfword or word), signed or unsigned operation, as well as special cases for operands A and B. The special cases that are permitted are replacing vector operand A with a scalar value, and replacing vector operand B with an enumerated vector. An enumerated vector provides an ordinal number for each element indicating its position in the vector.</p>
<h2 id="vector_lanes">Vector Lanes</h2>
<div class="figure">
<img src="images/mxp-scratchpad.png" alt="Quadruple-ported Scratchpad and Addressing" /><p class="caption">Quadruple-ported Scratchpad and Addressing</p>
</div>
<p>Calculations within MXP are performed by parallel 32-bit vector lanes. A processor configuration that is referred to as MXP-V4, for example, consists of four parallel vector lanes. Likewise, a V1 contains just one vector lane and V16 contains sixteen vector lanes.</p>
<p>The vector lanes can be instructed to operate upon three different data sizes: bytes (8 bits), halfwords (16 bits), or words (32 bits). When operating on smaller data sizes, the amount of parallelism increases proportionately. That is, a V4 configuration can perform either 16 parallel byte-size operations per cycle, or 8 parallel halfword-size operations per cycle.</p>
<p>Each lane contains a slice of the scratchpad memory, so wider vector processors are provided with a wider scratchpad memory. This scales the available memory bandwidth in a natural way to match the compute capacity of the MXP vector lanes.</p>
<p>As shown in the figure above, the scratchpad contains four access ports, and each is byte-addressable. Two dedicated read ports and one dedicated write port allows the processor to read its operands and writeback a result every clock cycle. The fourth port, a DMA port, can be dynamically configured to either read or write data. The DMA system can access the scratchpad without interrupting a computation in progress.</p>
<p>Furthermore, addresses in the scratchpad are striped across the vector lanes, in a similar way that data blocks are striped across disks in RAID-0. Each lane can store one word, two halfwords, or four bytes. Hence, scratchpad addresses increase by 4 when crossing from one lane to an adjacent lane.</p>
<p>Unlike a typical register-based processor, there is no preset limit imposed by the MXP instruction set architecture (ISA) on the number of vectors or the vector length. However, the size of the scratchpad forms a practical upper bound on the vector length and various other matrix parameters. Hence, at one extreme, the entire scratchpad can be filled by a single vector of bytes. At the other extreme, the entire scratchpad can be filled entirely by vectors that are each one byte long.</p>
<div class="figure">
<img src="images/mxp-arch.png" alt="VectorBlox MXP Vector Engine" /><p class="caption">VectorBlox MXP Vector Engine</p>
</div>
<p>While the ISA does not limit the number of vectors stored in the scratchpad, it is important to note that all scratchpad pointers are calculated and stored by the host scalar processor. Since the scalar register file is fixed in size and the operation of many registers are predetermined by its compiler application binary interface (ABI) specifications, using more than 8 to 16 vectors will likely involve spilling contents of the scalar register file to main memory.</p>
<h1 id="programming_model">Programming Model</h1>
<p>Programs written for the VectorBlox MXP use a combination of ANSI-C with VectorBlox C extensions. Basic ANSI-standard C code is run entirely on the host scalar processor. The VectorBlox C extensions are used to to specify the data-parallel operations that are to be computed by the MXP vector engine. These extensions are especially useful for image processing and similar applications where operations are applied to sets of data (such as pixels).</p>
<p>The VectorBlox MXP Matrix Processor provides two engines that run completely in parallel with the host processor: the MXP DMA engine, and the MXP vector engine.</p>
<h2 id="mxp_dma_engine">MXP DMA Engine</h2>
<p>The MXP direct-memory access (DMA) engine provides a means to perform block data transfers between the host’s external memory and the vector scratchpad. These transfers are performed asynchronously from the host processor.</p>
<p>The contents of the scratchpad are not managed automatically like a cache. Instead, the programmer must explicitly transfer memory from the host to the vector scratchpad, or vice versa.</p>
<p>Only one DMA transfer can be active at a time. Additional requests are queued by the system until the active transfer is complete. To ensure correctness, DMA transfers are always executed in FIFO or program order. Since DMA transfers can be interleaved with vector instructions, they are deposited into a common <strong><em>Instruction and DMA Request Queue</em></strong>. The MXP processor will automatically allow DMA transfers to bypass in-flight instructions, or vice versa, provided there are no data hazards between them. These data hazards are determined exclusively by the scratchpad address(es) being read or written. When a hazard exists, the MXP processor will insert a bubble into the pipeline, allowing current operations to complete before allowing a hazardous operation to make forward progress.</p>
<h2 id="mxp_vector_engine">MXP Vector Engine</h2>
<p>The MXP vector engine operates naturally on 8 bit bytes, 16 bit halfwords, or 32 bit words. The engine is organized into a set of vector lanes, where refer to the size of the vector engine as V1 or V128 for 1 or 128 vector lanes, respectively. The number of lanes must be a power of 2, where each lane incorporates a 32-bit ALU and a 32-bit slice of the scratchpad. Each vector lane can be subdivided on an instruction-by-instruction basis into two halflanes which operate on halfwords, or four bytelanes which operate on bytes. Thus, maximum parallelism is provided on byte-wide data.</p>
<p>The vector lanes of the MXP vector engine are only one means of achieving parallel execution. Even a single-lane MXP V1 with one 32-bit ALU can provide speedups over 20x compared to the host processor. This parallelism may come from many sources:</p>
<ul>
<li><p>byte or halfword parallelism</p></li>
<li><p>elimination of load and store instructions from the instruction stream</p></li>
<li><p>double-buffered asynchronous DMA operations overlap memory latency with computation</p></li>
<li><p>hardware-based loop counters and auto-incrementing address arithmetic</p></li>
<li><p>hardware-based looping avoids branch mispredictions</p></li>
<li><p>overlapped scalar instructions with vector instructions and DMA operations</p></li>
</ul>
<h2 id="mxp_programming_overview">MXP Programming Overview</h2>
<p>The general process for programming the MXP is:</p>
<ol style="list-style-type: decimal">
<li><p>Allocate vectors in scratchpad.</p></li>
<li><p>Transfer data from memory to scratchpad.</p></li>
<li><p>Operate on vectors in scratchpad.</p></li>
<li><p>Transfer data from scratchpad to memory.</p></li>
<li><p>Deallocate vectors in scratchpad.</p></li>
</ol>
<p>The scratchpad is a region of fast, on-chip parallel memory for holding and operating upon vector data. A minimum of 4kB per vector lane is provided, and this is usually enough for most applications.</p>
<p>The scratchpad is addressable by the Nios II processor, but care must be taken whether to cache the scratchpad or not. By default, scratchpad addresses are cached.</p>
<p>A simple vector program which adds three vectors is provided below:</p>
<h3 id="simple">Simple Example</h3>
<pre class="sourceCode c"><code class="sourceCode c">
<span class="ot">#include &quot;vbx.h&quot;</span>

<span class="dt">int</span> A[] = {<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>};
<span class="dt">int</span> B[] = {<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>};
<span class="dt">int</span> C[] = {-<span class="dv">1</span>, -<span class="dv">1</span>, -<span class="dv">1</span>, -<span class="dv">1</span>};

<span class="dt">int</span> main()
{
    <span class="dt">int</span> vector_len = <span class="dv">4</span>;
    <span class="dt">int</span> num_bytes = vector_len * <span class="kw">sizeof</span>(<span class="dt">int</span>);

    <span class="co">/* step 1 */</span>
    vbx_word_t * v_a = vbx_sp_malloc( num_bytes );
    vbx_word_t * v_b = vbx_sp_malloc( num_bytes );
    vbx_word_t * v_c = vbx_sp_malloc( num_bytes );

    <span class="co">/* step 2 */</span>
    vbx_dma_to_vector( v_a, A, num_bytes );
    vbx_dma_to_vector( v_b, B, num_bytes );

    <span class="co">/* step 3 */</span>
    vbx_set_vl( vector_len );
    vbx( VVW, VADD, v_c, v_a, v_b );

    <span class="co">/* step 4 */</span>
    vbx_dma_to_host( C, v_c, num_bytes );

    <span class="co">/* step 5 */</span>
    vbx_sp_free();
    printf( <span class="st">&quot;C[] = %d, %d, %d, %d</span><span class="ch">\n</span><span class="st">&quot;</span>,
    C[<span class="dv">0</span>], C[<span class="dv">1</span>], C[<span class="dv">2</span>], C[<span class="dv">3</span>] );
    
    <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre>
<p>Examples using dynamic allocation can be found in the <a href="#data_sharing">data-sharing section</a></p>
<h1 id="microblaze_programming">MicroBlaze Programming</h1>
<p>The MicroBlaze host processor has the following properties:</p>
<ul>
<li><p>3- or 5-stage pipeline, roughly 1 instruction per clock cycle</p></li>
<li><p>single-issue, in-order execution</p></li>
<li><p>4GB address space</p></li>
<li><p>optional direct-mapped, noncoherent caches</p></li>
</ul>
<p>On the Digilent Atlys demonstration systems, VectorBlox has configured the MicroBlaze as follows:</p>
<ul>
<li><p>5-stage pipeline</p></li>
<li><p>8kB instruction cache with 8-word cache lines</p></li>
<li><p>4kB write-back data cache with 8-word cache lines</p></li>
<li><p>no MMU</p></li>
<li><p>hardware multiplier (one cycle latency)</p></li>
<li><p>hardware barrel shifter</p></li>
<li><p>support for pattern compare instructions</p></li>
<li><p>branch target cache with 256 entries</p></li>
</ul>
<h2 id="microblaze_caching">MicroBlaze Caching</h2>
<p>MicroBlaze does not provide any mechanism for hardware cache coherence. Hence, programmers must manually flush the instruction or data cache when necessary. Flushing consists of writing back a data cache line (if dirty), and then invalidating the cache line.</p>
<p>Xilinx’s Standalone BSP provides two functions to flush the data cache:</p>
<ul>
<li><p><code>#include &quot;xil_cache.h&quot;</code> (required header file)</p></li>
<li><p><code>Xil_DCacheFlush()</code> flushes the entire data cache.</p></li>
<li><p><code>Xil_DCacheFlushRange(Addr, Len)</code> flushes the specified memory region from the data cache.</p></li>
</ul>
<p>Similarly, the instruction cache can be invalidated with</p>
<ul>
<li><p><code>Xil_ICacheInvalidate()</code>, or</p></li>
<li><p><code>Xil_ICacheInvalidateRange(Addr, Len)</code></p></li>
</ul>
<p>Note that <code>Xil_DCacheFlushRange()</code> must walk through the entire memory region of <code>Len</code> bytes (incrementing by the cache line size in the inner loop). For a sufficiently large memory region, it will be faster to just flush the entire data cache and start over.</p>
<h2 id="vbx_portability_library">VBX Portability Library</h2>
<p>To simplify the porting of MXP programs between different scalar host CPUs (e.g. between MicroBlaze and Nios II), VectorBlox provides a common cache management and timestamp timer API.</p>
<p>The cache management calls are similar to Altera’s API for the Nios II CPU. They assume that memory accesses can bypass the data cache by setting address bit 31 to 1, as is the case for a Nios II. MicroBlaze does <em>not</em> have an equivalent built-in mechanism to bypass the data cache, but the same functionality can be added by adding some bus connections and placing some restrictions on the MicroBlaze’s address map. Details can be found in the <em>MXP-M Quickstart Guide</em>.</p>
<p>The functions in the portability library are listed below. More details can be found in the <strong>VectorBlox MXP Reference</strong>.</p>
<pre class="sourceCode c"><code class="sourceCode c">    vbx_timestamp_start()
    vbx_timestamp_stop()
    vbx_timestamp_freq()
    vbx_timestamp()
    vbx_uncached_malloc()
    vbx_uncached_free()
    vbx_dcache_flush_all()
    vbx_dcache_flush(PTR,len) or vbx_dcache_flush_line(PTR)
    vbx_remap_cached(PTR,len)
    vbx_remap_uncached(PTR) or vbx_remap_uncached_flush(PTR,len)</code></pre>
<ul>
<li><p><code>vbx_dcache_flush(PTR,len)</code> considers the length of data being flushed. If it is too large, it will flush the entire data cache instead. Xilinx’s version loops over the entire length of the data.</p></li>
<li><p><code>vbx_remap_uncached(PTR)</code> remaps and flushes only a single cache line</p></li>
<li><p><code>vbx_remap_uncached_flush(PTR,len)</code> remaps and flushes a region, but the region is flushed using <code>vbx_dcache_flush(PTR,len)</code></p></li>
</ul>
<p>The functions below convert a pointer between cached and uncached mode by toggling bit 31:</p>
<ul>
<li><p><code>void *vbx_remap_cached(volatile void *ptr, u32 len)</code> returns a cached pointer.</p></li>
<li><p><code>volatile void *vbx_remap_uncached_flush(void *ptr, u32 len)</code> returns an uncached pointer. When mapping to uncached, the memory region will also be flushed.</p></li>
</ul>
<p>The functions below allocate and deallocate uncached memory:</p>
<ul>
<li><p><code>volatile void *vbx_uncached_malloc (size_t size)</code> allocates memory, returns an uncached pointer.</p></li>
<li><p><code>void vbx_uncached_free(volatile void *ptr)</code> unallocates an uncached region.</p></li>
</ul>
<h1 id="data_sharing">Data Sharing</h1>
<p>The code in the following subsections are variations of the vector addition code given earlier ( <a href="#simple">simple example</a>) using the VBX Portability Library.</p>
<p>The original code has no caching issues because the data is statically allocated in the data segment, so it is initialized into the data segment and untouched by the processor. However, the new code uses the processor to dynamically allocate and initialize the data, hence a copy of the data may be residing in the cache.</p>
<p>However, the <a href="#flushed">first example</a> shows one approach of managing cache coherence by flushing the shared regions out of the data cache. The source memory regions, <code>A</code> and <code>B</code>, must be flushed before the DMA transfer because the CPU has modified their values and dirty lines are being retained in the write-back cache. The memory region for the final answer, <code>C</code>, must be flushed because a stale copy or dirty copy may be in the data cache. Even if the CPU did not initialize the array <code>C</code>, elements at the beginning or end of C may have been brought into the cache because of locality and the sharing of cache lines with adjacent data. As a result, it can be difficult to keep track of whether to flush the data.</p>
<p>Instead, the <a href="#uncached">second example</a> shows our preferred approach of managing cache coherence. It works by marking shared regions as uncached data during allocation. Then, all scalar accesses will be to uncached data, but there will not be any coherence issues. This is the easiest way to program, but it may result in performance issues when the scalar processor initializes shared inputs or reads shared results. We still recommend this style, but suggest that you address performance issues by explicitly switching to a cached pointer where necessary. This is shown for array <code>A</code> in <a href="#cached">the final example</a>.</p>
<h2 id="data_sharing_examples">Data Sharing Examples</h2>
<h3 id="flushed">Example showing flushing of cached regions</h3>
<pre class="sourceCode c"><code class="sourceCode c">
<span class="ot">#include &quot;vbx.h&quot;</span>
<span class="ot">#include &quot;vbx_port.h&quot;</span>

<span class="dt">int</span> main()
{
    <span class="dt">int</span> A[] = {<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>};
    <span class="dt">int</span> B[] = {<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>};
    <span class="dt">int</span> C[] = {-<span class="dv">1</span>, -<span class="dv">1</span>, -<span class="dv">1</span>, -<span class="dv">1</span>};

    <span class="dt">int</span> vector_len = <span class="dv">4</span>;
    <span class="dt">int</span> num_bytes = vector_len * <span class="kw">sizeof</span>(<span class="dt">int</span>);

    <span class="co">/* step 1 */</span>
    vbx_word_t *va = vbx_sp_malloc( num_bytes );
    vbx_word_t *vb = vbx_sp_malloc( num_bytes );
    vbx_word_t *vc = vbx_sp_malloc( num_bytes );

    <span class="co">/* step 2 */</span>
    vbx_dcache_flush( A, num_bytes );
    vbx_dcache_flush( B, num_bytes );
    vbx_dma_to_vector( va, A, num_bytes );
    vbx_dma_to_vector( vb, B, num_bytes );

    <span class="co">/* step 3 */</span>
    vbx_set_vl( vector_len );
    vbx( VVW, VADD, vc, va, vb );

    <span class="co">/* step 4 */</span>
    vbx_dcache_flush( C, num_bytes );
    vbx_dma_to_host( C, vc, num_bytes );

    <span class="co">/* step 5 */</span>
    vbx_sp_free();

    printf( <span class="st">&quot;C[] = %d, %d, %d, %d</span><span class="ch">\n</span><span class="st">&quot;</span>,
             C[<span class="dv">0</span>], C[<span class="dv">1</span>], C[<span class="dv">2</span>], C[<span class="dv">3</span>] );
    <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre>
<h3 id="uncached">Example with shared regions allocated as uncached</h3>
<pre class="sourceCode c"><code class="sourceCode c">
<span class="ot">#include &quot;vbx.h&quot;</span>

<span class="dt">int</span> main()
{
    <span class="dt">int</span> vector_len = <span class="dv">4</span>;
    <span class="dt">int</span> num_bytes = vector_len * <span class="kw">sizeof</span>(<span class="dt">int</span>);

    <span class="dt">int</span> *A; A = vbx_shared_malloc( num_bytes );
    <span class="dt">int</span> *B; B = vbx_shared_malloc( num_bytes );
    <span class="dt">int</span> *C; C = vbx_shared_malloc( num_bytes );

    A[<span class="dv">0</span>] = <span class="dv">1</span>; A[<span class="dv">1</span>] = <span class="dv">2</span>; A[<span class="dv">2</span>] = <span class="dv">3</span>; A[<span class="dv">3</span>] = <span class="dv">4</span>;
    B[<span class="dv">0</span>] = <span class="dv">5</span>; B[<span class="dv">1</span>] = <span class="dv">6</span>; B[<span class="dv">2</span>] = <span class="dv">7</span>; B[<span class="dv">3</span>] = <span class="dv">8</span>;

    <span class="co">/* step 1 */</span>
    vbx_word_t *va = vbx_sp_malloc( num_bytes );
    vbx_word_t *vb = vbx_sp_malloc( num_bytes );
    vbx_word_t *vc = vbx_sp_malloc( num_bytes );

    <span class="co">/* step 2 */</span>
    vbx_dma_to_vector( va, A, num_bytes );
    vbx_dma_to_vector( vb, B, num_bytes );

    <span class="co">/* step 3 */</span>
    vbx_set_vl( vector_len );
    vbx( VVW, VADD, vc, va, vb );

    <span class="co">/* step 4 */</span>
    vbx_dma_to_host( C, vc, num_bytes );

    <span class="co">/* step 5 */</span>
    vbx_sp_free();

    printf( <span class="st">&quot;C[] = %d, %d, %d, %d</span><span class="ch">\n</span><span class="st">&quot;</span>,
             C[<span class="dv">0</span>], C[<span class="dv">1</span>], C[<span class="dv">2</span>], C[<span class="dv">3</span>] );
    <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre>
<h3 id="cached">Example combining shared (uncached) and cached regions</h3>
<pre class="sourceCode c"><code class="sourceCode c">
<span class="ot">#include &quot;vbx.h&quot;</span>
<span class="ot">#include &quot;vbx_port.h&quot;</span>

<span class="dt">int</span> main()
{
    <span class="dt">int</span> vector_len = <span class="dv">4</span>;
    <span class="dt">int</span> num_bytes = vector_len * <span class="kw">sizeof</span>(<span class="dt">int</span>);

    <span class="dt">int</span> *A; A = vbx_shared_malloc( num_bytes );
    <span class="dt">int</span> *B; B = vbx_shared_malloc( num_bytes );
    <span class="dt">int</span> *C; C = vbx_shared_malloc( num_bytes );

    <span class="dt">int</span> *cachedA = vbx_remap_cached( A, num_bytes );
    cachedA[<span class="dv">0</span>] = <span class="dv">1</span>; cachedA[<span class="dv">1</span>] = <span class="dv">2</span>;
    cachedA[<span class="dv">2</span>] = <span class="dv">3</span>; cachedA[<span class="dv">3</span>] = <span class="dv">4</span>; <span class="co">// cached, faster</span>
    vbx_dcache_flush( cachedA, num_bytes );
    B[<span class="dv">0</span>] = <span class="dv">5</span>; B[<span class="dv">1</span>] = <span class="dv">6</span>;
    B[<span class="dv">2</span>] = <span class="dv">7</span>; B[<span class="dv">3</span>] = <span class="dv">8</span>; <span class="co">// uncached, slower</span>

    vbx_word_t *va = vbx_sp_malloc( num_bytes );
    vbx_word_t *vb = vbx_sp_malloc( num_bytes );
    vbx_word_t *vc = vbx_sp_malloc( num_bytes );
  
    vbx_dma_to_vector( va, A, num_bytes );
    vbx_dma_to_vector( vb, B, num_bytes );
  
    vbx_set_vl( vector_len );
    vbx( VVW, VADD, vc, va, vb );
  
    vbx_dma_to_host( C, vc, num_bytes );
  
    vbx_sp_free();
  
    printf( <span class="st">&quot;C[] = %d, %d, %d, %d</span><span class="ch">\n</span><span class="st">&quot;</span>,
             C[<span class="dv">0</span>], C[<span class="dv">1</span>], C[<span class="dv">2</span>], C[<span class="dv">3</span>] );
    <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre>
</div>
</div>
</div>
<script src="js/jquery-1.8.3.min.js"></script>
<script src="js/jquery-ui-1.9.1.custom.min.js"></script>
<script src="js/bootstrap.js"></script>
<script src="js/jquery.tocify.min.js"></script>
<script>
$(function() {
    var toc = $("#toc").tocify({
        selectors: "h1, h2",
        history: false,
        smoothScrollSpeed: "fast",
        }).data("toc-tocify");
      $(".optionName").popover({ trigger: "hover" });
});
</script>
</body>
</html>
